# -*- coding: utf-8 -*-
"""Diksha app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f1et7IjiG3Aof1NXSAWo83UwNa4Nrq9s
"""

#1.Load the Dataset & Check
import pandas as pd

# Load the dataset directly from UCI (hosted as .data file)
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data"
columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']
df = pd.read_csv(url, names=columns)

# Basic info
print("Shape of the dataset:", df.shape)
print("\nFirst few rows:")
print(df.head())
print("\nData types and non-null counts:")
print(df.info())
print("\nMissing values in each column:")
print(df.isnull().sum())

#2: Data Cleaning
#Handle Missing Values
#We already checked — there are no missing values, so we can skip imputation.

#Convert Categorical Variables to Numeric



from sklearn.preprocessing import LabelEncoder

# Create a copy to preserve original
df_cleaned = df.copy()

# Apply Label Encoding
le = LabelEncoder()
for col in df_cleaned.columns:
    df_cleaned[col] = le.fit_transform(df_cleaned[col])

# Check the transformed dataset
print(df_cleaned.head())

#3: Target Variable Understanding
#Meaning of Target Values

#The class column (target variable) contains the following categories:

#unacc: unacceptable

#acc: acceptable

#good: good

#vgood: very good

#It’s a multi-class classification problem with 4 classes.



# Original (non-encoded) value counts
print("Target distribution:")
print(df['class'].value_counts())

#3.2 Encode Target Variable
#We’ll convert the class column into numeric labels.
# Encode target separately
target_encoder = LabelEncoder()
df_cleaned['class'] = target_encoder.fit_transform(df['class'])

# Map of encoded values
target_mapping = dict(zip(target_encoder.classes_, target_encoder.transform(target_encoder.classes_)))
print("Target mapping:", target_mapping)

#4: Visualization
#Correlation Heatmap
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(6,6))
sns.heatmap(df_cleaned.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

#Note: Since all features are encoded from categorical, the correlation might not provide deep insight but still gives a relative sense of influence.



#4.2 Target vs Features
# Convert target column back to original labels for better visualization
df_viz = df.copy()

features = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']
for feature in features:
    plt.figure(figsize=(6,4))
    sns.countplot(x=feature, hue='class', data=df_viz)
    plt.title(f"{feature.capitalize()} vs Class")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

#5: Model Training
from sklearn.model_selection import train_test_split

X = df_cleaned.drop('class', axis=1)
y = df_cleaned['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

def evaluate_model(model):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"Model: {model.__class__.__name__}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision (macro):", precision_score(y_test, y_pred, average='macro'))
    print("Recall (macro):", recall_score(y_test, y_pred, average='macro'))
    print("F1 Score (macro):", f1_score(y_test, y_pred, average='macro'))
    print("\nClassification Report:\n", classification_report(y_test, y_pred))
    print("-" * 50)

# Logistic Regression
lr = LogisticRegression(max_iter=1000)
evaluate_model(lr)

# Random Forest
rf = RandomForestClassifier(random_state=42)
evaluate_model(rf)

#6: Feature Importance
import pandas as pd

# Fit model
rf.fit(X_train, y_train)

# Get feature importances
importances = rf.feature_importances_
feature_names = X.columns

# Create DataFrame for better visualization
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(8,5))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title("Feature Importance from Random Forest")
plt.tight_layout()
plt.show()

#7: Model Comparison
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Define all models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42)
}

# Store results
results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    f1 = f1_score(y_test, y_pred, average='macro')

    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    })

# Create DataFrame
results_df = pd.DataFrame(results).sort_values(by='F1 Score', ascending=False)
print(results_df)